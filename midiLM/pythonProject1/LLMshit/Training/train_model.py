import matplotlib.pyplot as plt
import os
from transformers import TrainingArguments, Trainer
from datasets import Dataset
from transformers import DataCollatorForLanguageModeling
from datetime import datetime
import json
from transformers import TrainerCallback
from transformers import AdamW


def train_model(model, tokenized_data, tokenizer, output_path, data_file_name):
    # ç¡®ä¿æ—¥å¿—å­˜å‚¨ç›®å½•å­˜åœ¨
    os.makedirs(os.path.join(output_path, "logs"), exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file_name = f"log_{data_file_name.replace('.json', '')}_{timestamp}.json"
    log_file_path = os.path.join(output_path, "logs", log_file_name)

    # **è½¬æ¢ tokenized_data ä¸º Dataset**
    dataset = Dataset.from_list([{"input_ids": entry["input_ids"]} for entry in tokenized_data])

    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
    training_metrics = []  # å­˜ loss æ•°æ®

    # âœ… **å®šä¹‰ LogCallback å¹¶æ­£ç¡®æ³¨å†Œ**
    class LogCallback(TrainerCallback):
        """
        è‡ªå®šä¹‰ Trainer å›è°ƒï¼Œç”¨äºè®°å½• loss å¹¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ã€‚
        """

        def __init__(self, log_file_path):
            self.log_file_path = log_file_path
            self.training_metrics = []  # å­˜ loss æ•°æ®

        def on_log(self, args, state, control, logs=None, **kwargs):
            """
            è®°å½• lossï¼Œå¹¶ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶ã€‚
            """
            if logs and "loss" in logs:
                self.training_metrics.append({"step": state.global_step, "loss": logs["loss"]})
                with open(self.log_file_path, "w", encoding="utf-8") as f:
                    json.dump(self.training_metrics, f, indent=2)

        def on_init_end(self, args, state, control, **kwargs):
            """åˆå§‹åŒ– Trainer æ—¶è°ƒç”¨"""
            print("âœ… LogCallback åˆå§‹åŒ–å®Œæˆï¼")

        def on_train_begin(self, args, state, control, **kwargs):
            """è®­ç»ƒå¼€å§‹æ—¶è°ƒç”¨"""
            print(f"ğŸš€ è®­ç»ƒå¼€å§‹ï¼Œæ€»æ­¥æ•°: {state.max_steps}")

        def on_train_end(self, args, state, control, **kwargs):
            """è®­ç»ƒç»“æŸæ—¶è°ƒç”¨"""
            print(f"ğŸ‰ è®­ç»ƒå®Œæˆï¼Œæ€»æ­¥æ•°: {state.global_step}")

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # åˆ›å»ºæ—¥å¿—è·¯å¾„
    log_file_name = f"log_{data_file_name.replace('.json', '')}_{timestamp}.json"
    log_file_path = os.path.join(output_path, "logs", log_file_name)

    log_callback = LogCallback(log_file_path)  # âœ… ä¼ å…¥æ—¥å¿—æ–‡ä»¶è·¯å¾„

    # **è®­ç»ƒå‚æ•°**
    training_args = TrainingArguments(
        output_dir=output_path,
        overwrite_output_dir=True,

        # **1ï¸âƒ£ è®­ç»ƒç­–ç•¥**
        num_train_epochs=4,  # ğŸ”¹ **å‡å°‘ epochï¼Œé¿å…è¿‡æ‹Ÿåˆ**
        per_device_train_batch_size=16,  # ğŸ”¹ **æ˜¾å­˜è¶³å¤Ÿï¼Œbatch 16**
        gradient_accumulation_steps=4,  # ğŸ”¹ **å…¨å±€ batch size = 16Ã—4=64**

        # **2ï¸âƒ£ ä¼˜åŒ–æ˜¾å­˜**
        save_steps=2000,  # ğŸ”¹ **å‡å°‘ checkpoint é¢‘ç‡ï¼Œé™ä½ IO å½±å“**
        save_total_limit=2,  # ğŸ”¹ **æœ€å¤šä¿ç•™ 2 ä¸ª checkpoint**
        fp16=True,  # ğŸ”¹ **å¯ç”¨æ··åˆç²¾åº¦ï¼Œé™ä½æ˜¾å­˜å ç”¨**

        # **3ï¸âƒ£ å­¦ä¹ ç‡ & è®­ç»ƒä¼˜åŒ–**
        learning_rate=3e-4,  # ğŸ”¹ **æ›´é«˜çš„å­¦ä¹ ç‡ï¼ˆé€‚åº”å°æ•°æ®ï¼‰**
        # lr_scheduler_type="cosine",  # ğŸ”¹ **æ›´å¹³ç¨³çš„è¡°å‡**
        warmup_steps=1000,  # ğŸ”¹ **å‡å°‘ warmupï¼Œå¿«é€Ÿè¿›å…¥æ”¶æ•›**
        weight_decay=0.1,  # ğŸ”¹ **æ›´é«˜çš„æ­£åˆ™åŒ–ï¼Œæé«˜æ³›åŒ–**

        # **4ï¸âƒ£ æ—¥å¿— & ç›‘æ§**
        logging_dir=os.path.join(output_path, "logs"),
        logging_steps=100,  # ğŸ”¹ **å‡å°‘æ—¥å¿—é¢‘ç‡**
        save_strategy="epoch",  # ğŸ”¹ **æ¯ä¸ª epoch å­˜ checkpoint**
        report_to="none",
    )

    optimizer = AdamW(
        model.parameters(),
        lr=3e-4,  # åˆå§‹å­¦ä¹ ç‡
        weight_decay=0.1,  # æƒé‡è¡°å‡ï¼ˆå¢å¼ºæ³›åŒ–ï¼‰
        betas=(0.9, 0.95)  # åŠ¨é‡å‚æ•°ï¼ˆé€‚åˆ LLMï¼‰
    )

    # âœ… **å°† log_callback ç»‘å®šåˆ° Trainer**
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        optimizers=(optimizer, None),
        callbacks=[log_callback],  # âœ… è¿™é‡Œæ³¨å†Œå›è°ƒ
    )

    # **è®­ç»ƒæ¨¡å‹**
    trainer.train()

    # **ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨**
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)

    print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜è‡³ {output_path}")
    print(f"ğŸ“‚ è®­ç»ƒæ—¥å¿—ä¿å­˜è‡³ {log_file_path}")

    # âœ… **ç»˜åˆ¶ Loss æ›²çº¿**
    if training_metrics:
        steps = [entry["step"] for entry in training_metrics]
        losses = [entry["loss"] for entry in training_metrics]

        plt.figure(figsize=(8, 5))
        plt.plot(steps, losses, label="Training Loss", color="blue")
        plt.xlabel("Steps")
        plt.ylabel("Loss")
        plt.title("Training Loss Curve")
        plt.legend()
        plt.grid(True)
        plt.savefig(os.path.join(output_path, "loss_curve.png"))  # ä¿å­˜ loss æ›²çº¿
        plt.show()

    return log_file_path
