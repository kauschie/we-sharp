from transformers import PreTrainedTokenizerFast

# 加载分词器
tokenizer = PreTrainedTokenizerFast(tokenizer_file="Tokenizers/tokenizer0.json")

# 测试分词器
text = " 35 530 945 1010 598 1001 530 945 890 1010 807 1001 530 945 945 1010 598 1001 1001 955 424 945 945 563 807 70 424 945 945 563 807 70 530 945 945 563 807 1001 1001 945 670 164 598 1001 811 136 860 672 321 321 670 293 967 310 957 900 617 140 900 945 307 563 811 756 164 530 670 811 563 310 70 670 164 900 310 497 300 957 131 321 860 670 310 1001 670 80 900 687 136 900 164 875 670 860 534 380 875 936 530 900 164 136 971 811 955 491 131 670 164 900 991 811 900 164 719 70 811 945 945 563 807 70 955 945 945 670 80 942 224 583 900 617 108 807 811 570 530 957 534 321 672 875 811 945 991 563 756 1001 570 251 811 900 605 672 871 811 255 164 942 719 651 672 321 811 900 957 900 310 131 35 628 563 157 598 164 224 80 811 811 696 811 696 875 563 875 670 670"
tokenized = tokenizer(text)
print("Tokenized:", tokenized.input_ids)
